# MindSearch

A modern **Retrieval-Augmented Generation (RAG)** system that enables intelligent document chat using semantic search and LLM integration.

## Features

- ğŸ“„ **Multi-format Document Support**: Ingest PDF, DOCX, and TXT files
- ğŸ” **Semantic Search**: Uses sentence transformers and FAISS for efficient vector search
- ğŸ’¬ **AI-Powered Chat**: Stream responses from Ollama LLM models
- ğŸš€ **Production-Ready**: FastAPI backend with Streamlit frontend
- âš¡ **Async Streaming**: Server-Sent Events (SSE) for real-time responses

## Architecture

```
Frontend (Streamlit)
      â†“
FastAPI Backend
      â”œâ”€â”€ Document Ingestion â†’ Chunking â†’ Embedding
      â”œâ”€â”€ Vector Search (FAISS)
      â””â”€â”€ LLM Integration (Ollama)
```

## Prerequisites

- Python 3.9+
- Ollama with `llama3:8b` model installed ([ollama.ai](https://ollama.ai))
- pip

## Installation

### 1. Clone the repository
```bash
git clone https://github.com/YOUR_USERNAME/MindSearch.git
cd MindSearch
```

### 2. Create virtual environment
```bash
python -m venv venv
# Windows
venv\Scripts\activate
# macOS/Linux
source venv/bin/activate
```

### 3. Install dependencies
```bash
pip install -r backend/requirements.txt
```

## Usage

### 1. Start Ollama Service
```bash
ollama serve
# In another terminal, pull the model if needed:
ollama pull llama3:8b
```

### 2. Start Backend (FastAPI)
```bash
cd backend
python -m uvicorn main:app --reload
# Server runs on http://localhost:8000
```

### 3. Start Frontend (Streamlit)
```bash
cd frontend
streamlit run app.py
# Opens on http://localhost:8501
```

## API Endpoints

### Chat Completion (Streaming)
```bash
POST /v1/chat/completions
Content-Type: application/json

{
  "model": "rag-model",
  "messages": [{"role": "user", "content": "What is in the documents?"}],
  "stream": true,
  "session_id": "session-123"
}
```

### Document Ingestion
```bash
POST /v1/ingest
Content-Type: multipart/form-data

session_id: session-123
files: [file1.pdf, file2.txt, ...]
```

## Project Structure

```
MindSearch/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI app
â”‚   â”œâ”€â”€ rag_pipeline.py      # Core RAG logic
â”‚   â”œâ”€â”€ chunker.py           # Document processing
â”‚   â”œâ”€â”€ embedder.py          # Vector embeddings (FAISS)
â”‚   â”œâ”€â”€ retriever.py         # Semantic search
â”‚   â”œâ”€â”€ llm.py               # LLM streaming
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ uploads/             # Uploaded documents
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py               # Streamlit UI
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## Key Components

### Document Chunking (`chunker.py`)
- Supports PDF, DOCX, TXT files
- Sentence-based chunking with configurable size
- Automatic encoding detection

### Embeddings (`embedder.py`)
- Uses `sentence-transformers` (all-MiniLM-L6-v2)
- FAISS for efficient similarity search
- Indexes stored for fast retrieval

### LLM Integration (`llm.py`)
- Streaming responses via Ollama
- Async/thread-safe implementation
- Error handling and fallbacks

## Configuration

Edit backend files to customize:
- **Chunk size**: `chunker.py` â†’ `chunk_size` parameter
- **Model name**: `llm.py` â†’ `stream_generate()` model parameter
- **Search results**: `retriever.py` â†’ `k` parameter
- **API base URL**: `frontend/app.py` â†’ `API_BASE_URL`

## Dependencies

**Backend:**
- FastAPI, Uvicorn
- sentence-transformers, FAISS
- pdfplumber, python-docx
- Ollama client
- NLTK

**Frontend:**
- Streamlit
- requests

## Troubleshooting

### "Unable to connect to RAG API"
- Check if backend is running on `http://localhost:8000`
- Verify firewall settings

### "No documents ingested yet"
- Upload documents first using the frontend
- Check `uploads/` folder for saved files

### "Ollama connection error"
- Ensure Ollama service is running (`ollama serve`)
- Check if model is installed (`ollama list`)

### NLTK tokenizer errors
- Automatically downloaded on first use
- Manual download: `python -m nltk.downloader punkt_tab`

## Performance Tips

- Larger chunk sizes â†’ fewer but longer context windows
- Smaller embedding models â†’ faster but less accurate search
- Adjust `k` (search results) based on context length needs

## Future Enhancements

- [ ] Multi-model support (GPT-4, Claude)
- [ ] Web search integration
- [ ] Query expansion with knowledge graphs
- [ ] Document metadata filtering
- [ ] User authentication & persistence
- [ ] Docker deployment
- [ ] Evaluation metrics dashboard

## License

MIT

## Author

Nitin Dutt - [GitHub](https://github.com/nitin-dutt)

